{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.spatial\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gradient descent optimization\n",
    "# The learning rate is specified by eta\n",
    "class GDOptimizer(object):\n",
    "    def __init__(self, eta):\n",
    "        self.eta = eta\n",
    "\n",
    "    def initialize(self, layers):\n",
    "        pass\n",
    "\n",
    "    # This function performs one gradient descent step\n",
    "    # layers is a list of dense layers in the network\n",
    "    # g is a list of gradients going into each layer before the nonlinear activation\n",
    "    # a is a list of of the activations of each node in the previous layer going \n",
    "    def update(self, layers, g, a):\n",
    "        m = a[0].shape[1]\n",
    "        for layer, curGrad, curA in zip(layers, g, a):\n",
    "            update = np.dot(curGrad,curA.T)\n",
    "            updateB = np.sum(curGrad,1).reshape(layer.b.shape)\n",
    "            layer.updateWeights(-self.eta/m * np.dot(curGrad,curA.T))\n",
    "            layer.updateBias(-self.eta/m * np.sum(curGrad,1).reshape(layer.b.shape))\n",
    "\n",
    "# Cost function used to compute prediction errors\n",
    "class QuadraticCost(object):\n",
    "\n",
    "    # Compute the squared error between the prediction yp and the observation y\n",
    "    # This method should compute the cost per element such that the output is the\n",
    "    # same shape as y and yp\n",
    "    @staticmethod\n",
    "    def fx(y,yp):\n",
    "        return 0.5 * np.square(yp-y)\n",
    "\n",
    "    # Derivative of the cost function with respect to yp\n",
    "    @staticmethod\n",
    "    def dx(y,yp):\n",
    "        return y - yp\n",
    "\n",
    "# Sigmoid function fully implemented as an example\n",
    "class SigmoidActivation(object):\n",
    "    @staticmethod\n",
    "    def fx(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def dx(z):\n",
    "        return SigmoidActivation.fx(z) * (1 - SigmoidActivation.fx(z))\n",
    "        \n",
    "# Hyperbolic tangent function\n",
    "class TanhActivation(object):\n",
    "\n",
    "    # Compute tanh for each element in the input z\n",
    "    @staticmethod\n",
    "    def fx(z):\n",
    "        return np.tanh(z)\n",
    "\n",
    "    # Compute the derivative of the tanh function with respect to z\n",
    "    @staticmethod\n",
    "    def dx(z):\n",
    "        return 1 - np.square(np.tanh(z))\n",
    "\n",
    "# Rectified linear unit\n",
    "class ReLUActivation(object):\n",
    "    @staticmethod\n",
    "    def fx(z):\n",
    "        return np.maximum(0,z)\n",
    "\n",
    "    @staticmethod\n",
    "    def dx(z):\n",
    "        return (z>0).astype('float')\n",
    "\n",
    "# Linear activation\n",
    "class LinearActivation(object):\n",
    "    @staticmethod\n",
    "    def fx(z):\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def dx(z):\n",
    "        return np.ones(z.shape)\n",
    "\n",
    "# This class represents a single hidden or output layer in the neural network\n",
    "class DenseLayer(object):\n",
    "\n",
    "    # numNodes: number of hidden units in the layer\n",
    "    # activation: the activation function to use in this layer\n",
    "    def __init__(self, numNodes, activation):\n",
    "        self.numNodes = numNodes\n",
    "        self.activation = activation\n",
    "\n",
    "    def getNumNodes(self):\n",
    "        return self.numNodes\n",
    "\n",
    "    # Initialize the weight matrix of this layer based on the size of the matrix W\n",
    "    def initialize(self, fanIn, scale=1.0):\n",
    "        s = scale * np.sqrt(6.0 / (self.numNodes + fanIn))\n",
    "        self.W = np.random.normal(0, s,\n",
    "                                   (self.numNodes,fanIn))\n",
    "        #self.b = np.zeros((self.numNodes,1))\n",
    "        self.b = np.random.uniform(-1,1,(self.numNodes,1))\n",
    "\n",
    "    # Apply the activation function of the layer on the input z\n",
    "    def a(self, z):\n",
    "        return self.activation.fx(z)\n",
    "\n",
    "    # Compute the linear part of the layer\n",
    "    # The input a is an n x k matrix where n is the number of samples\n",
    "    # and k is the dimension of the previous layer (or the input to the network)\n",
    "    def z(self, a): \n",
    "        #print('a:\\n'+str(a))\n",
    "        #print('Wa:\\n'+str(self.W.dot(a)))\n",
    "        return self.W.dot(a) + self.b # Note, this is implemented where we assume a is k x n\n",
    "\n",
    "    # Compute the derivative of the layer's activation function with respect to z\n",
    "    # where z is the output of the above function.\n",
    "    # This derivative does not contain the derivative of the matrix multiplication\n",
    "    # in the layer.  That part is computed below in the model class.\n",
    "    def dx(self, z):\n",
    "        return self.activation.dx(z)\n",
    "\n",
    "    # Update the weights of the layer by adding dW to the weights\n",
    "    def updateWeights(self, dW):\n",
    "        self.W = self.W + dW\n",
    "\n",
    "    # Update the bias of the layer by adding db to the bias\n",
    "    def updateBias(self, db):\n",
    "        self.b = self.b + db\n",
    "\n",
    "# This class handles stacking layers together to form the completed neural network\n",
    "class Model(object):\n",
    "\n",
    "    # inputSize: the dimension of the inputs that go into the network\n",
    "    def __init__(self, inputSize):\n",
    "        self.layers = []\n",
    "        self.inputSize = inputSize\n",
    "\n",
    "    # Add a layer to the end of the network\n",
    "    def addLayer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Get the output size of the layer at the given index\n",
    "    def getLayerSize(self, index):\n",
    "        if index >= len(self.layers):\n",
    "            return self.layers[-1].getNumNodes()\n",
    "        elif index < 0:\n",
    "            return self.inputSize\n",
    "        else:\n",
    "            return self.layers[index].getNumNodes()\n",
    "\n",
    "    # Initialize the weights of all of the layers in the network and set the cost\n",
    "    # function to use for optimization\n",
    "    def initialize(self, cost, initializeLayers=True):\n",
    "        self.cost = cost\n",
    "        if initializeLayers:\n",
    "            for i in range(0,len(self.layers)):\n",
    "                if i == len(self.layers) - 1:\n",
    "                    self.layers[i].initialize(self.getLayerSize(i-1))\n",
    "                else:\n",
    "                    self.layers[i].initialize(self.getLayerSize(i-1))\n",
    "\n",
    "    # Compute the output of the network given some input a\n",
    "    # The matrix a has shape n x k where n is the number of samples and\n",
    "    # k is the dimension\n",
    "    # This function returns\n",
    "    # yp - the output of the network\n",
    "    # a - a list of inputs for each layer of the newtork where\n",
    "    #     a[i] is the input to layer i\n",
    "    # z - a list of values for each layer after evaluating layer.z(a) but\n",
    "    #     before evaluating the nonlinear function for the layer\n",
    "    def evaluate(self, x):\n",
    "        curA = x.T\n",
    "        a = [curA]\n",
    "        z = []\n",
    "        for layer in self.layers:\n",
    "            z.append(layer.z(curA))\n",
    "            curA = layer.a(z[-1])\n",
    "            a.append(curA)\n",
    "        yp = a.pop()\n",
    "        return yp, a, z\n",
    "\n",
    "    # Compute the output of the network given some input a\n",
    "    # The matrix a has shape n x k where n is the number of samples and\n",
    "    # k is the dimension\n",
    "    def predict(self, a):\n",
    "        a,_,_ = self.evaluate(a)\n",
    "        return a.T\n",
    "\n",
    "    # Train the network given the inputs x and the corresponding observations y\n",
    "    # The network should be trained for numEpochs iterations using the supplied\n",
    "    # optimizer\n",
    "    def train(self, x, y, numEpochs, optimizer):\n",
    "\n",
    "        # Initialize some stuff\n",
    "        n = x.shape[0]\n",
    "        hist = []\n",
    "        optimizer.initialize(self.layers)\n",
    "        \n",
    "        # Run for the specified number of epochs\n",
    "        for epoch in range(0,numEpochs):\n",
    "\n",
    "            # Feed forward\n",
    "            # Save the output of each layer in the list a\n",
    "            # After the network has been evaluated, a should contain the\n",
    "            # input x and the output of each layer except for the last layer\n",
    "            yp, a, z = self.evaluate(x)\n",
    "\n",
    "            # Compute the error\n",
    "            C = self.cost.fx(yp,y.T)\n",
    "            d = self.cost.dx(yp,y.T)\n",
    "            grad = []\n",
    "\n",
    "            # Backpropogate the error\n",
    "            idx = len(self.layers)\n",
    "            for layer, curZ in zip(reversed(self.layers),reversed(z)):\n",
    "                idx = idx - 1\n",
    "                # Here, we compute dMSE/dz_i because in the update\n",
    "                # function for the optimizer, we do not give it\n",
    "                # the z values we compute from evaluating the network\n",
    "                grad.insert(0,np.multiply(d,layer.dx(curZ)))\n",
    "                d = np.dot(layer.W.T,grad[0])\n",
    "\n",
    "            # Update the errors\n",
    "            optimizer.update(self.layers, grad, a)\n",
    "\n",
    "            # Compute the error at the end of the epoch\n",
    "            yh = self.predict(x)\n",
    "            C = self.cost.fx(yh,y)\n",
    "            C = np.mean(C)\n",
    "            hist.append(C)\n",
    "        return hist\n",
    "\n",
    "    def trainBatch(self, x, y, batchSize, numEpochs, optimizer):\n",
    "\n",
    "        # Copy the data so that we don't affect the original one when shuffling\n",
    "        x = x.copy()\n",
    "        y = y.copy()\n",
    "        hist = []\n",
    "        n = x.shape[0]\n",
    "        \n",
    "        for epoch in np.arange(0,numEpochs):\n",
    "            \n",
    "            # Shuffle the data\n",
    "            r = np.arange(0,x.shape[0])\n",
    "            x = x[r,:]\n",
    "            y = y[r,:]\n",
    "            e = []\n",
    "\n",
    "            # Split the data in chunks and run SGD\n",
    "            for i in range(0,n,batchSize):\n",
    "                end = min(i+batchSize,n)\n",
    "                batchX = x[i:end,:]\n",
    "                batchY = y[i:end,:]\n",
    "                e += self.train(batchX, batchY, 1, optimizer)\n",
    "            hist.append(np.mean(e))\n",
    "\n",
    "        return hist\n",
    "\n",
    "\n",
    "########################################################################\n",
    "######### Part b #######################################################\n",
    "########################################################################\n",
    "\n",
    "########################################################################\n",
    "#########  Gradient Computing and MLE ##################################\n",
    "########################################################################\n",
    "def compute_gradient_of_likelihood(single_obj_loc, sensor_loc, \n",
    "                                single_distance, noise = 1):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loglikelihood function for part a.   \n",
    "    \n",
    "    Input:\n",
    "    single_obj_loc: 1 * d numpy array. \n",
    "    Location of the single object.\n",
    "    \n",
    "    sensor_loc: k * d numpy array. \n",
    "    Location of sensor.\n",
    "    \n",
    "    single_distance: k dimensional numpy array. \n",
    "    Observed distance of the object.\n",
    "    \n",
    "    Output:\n",
    "    grad: d-dimensional numpy array.\n",
    "    \n",
    "    \"\"\"\n",
    "    loc_difference = single_obj_loc - sensor_loc # k * d.\n",
    "    phi = np.linalg.norm(loc_difference, axis = 1) # k. \n",
    "    weight = (phi - single_distance) / phi # k.\n",
    "    \n",
    "    grad = -np.sum(np.expand_dims(weight,1)*loc_difference, \n",
    "                   axis = 0)/noise ** 2 # d\n",
    "    return grad \n",
    "\n",
    "########################################################################\n",
    "######### Part c #################################################\n",
    "########################################################################\n",
    "def log_likelihood(obj_loc, sensor_loc, distance, noise = 1): \n",
    "    \"\"\"\n",
    "    This function computes the log likelihood (as expressed in Part a).\n",
    "    Input: \n",
    "    obj_loc: shape [1,2]\n",
    "    sensor_loc: shape [7,2]\n",
    "    distance: shape [7]\n",
    "    Output: \n",
    "    The log likelihood function value. \n",
    "    \"\"\"  \n",
    "    diff_distance = np.sqrt(np.sum((sensor_loc - obj_loc)**2, axis = 1))- distance\n",
    "    func_value = -sum((diff_distance)**2)/(2 * noise ** 2)\n",
    "    return func_value\n",
    " \n",
    "\n",
    "\n",
    "########################################################################\n",
    "######### Part e, f, g #################################################\n",
    "########################################################################\n",
    "\n",
    "########################################################################\n",
    "#########  Gradient Computing and MLE ##################################\n",
    "########################################################################\n",
    "def compute_grad_likelihood_part_e(sensor_loc, obj_loc, distance, noise = 1):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loglikelihood function for part d.   \n",
    "    \n",
    "    Input:\n",
    "    sensor_loc: k * d numpy array. \n",
    "    Location of sensors.\n",
    "    \n",
    "    obj_loc: n * d numpy array. \n",
    "    Location of the objects.\n",
    "    \n",
    "    distance: n * k dimensional numpy array. \n",
    "    Observed distance of the object.\n",
    "    \n",
    "    Output:\n",
    "    grad: k * d numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.zeros(sensor_loc.shape)\n",
    "    for i, single_sensor_loc in enumerate(sensor_loc):\n",
    "        single_distance = distance[:,i] \n",
    "        grad[i] = compute_gradient_of_likelihood(single_sensor_loc, \n",
    "                     obj_loc, single_distance, noise)\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def find_mle_by_grad_descent_part_e(initial_sensor_loc, \n",
    "           obj_loc, distance, noise = 1, lr=0.001, num_iters = 1000):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loglikelihood function for part a.   \n",
    "    \n",
    "    Input:\n",
    "    initial_sensor_loc: k * d numpy array. \n",
    "    Initialized Location of the sensors.\n",
    "    \n",
    "    obj_loc: n * d numpy array. Location of the n objects.\n",
    "    \n",
    "    distance: n * k dimensional numpy array. \n",
    "    Observed distance of the n object.\n",
    "    \n",
    "    Output:\n",
    "    sensor_loc: k * d numpy array. The mle for the location of the object.\n",
    "    \n",
    "    \"\"\"    \n",
    "    sensor_loc = initial_sensor_loc\n",
    "    for t in range(num_iters):\n",
    "        sensor_loc += lr * compute_grad_likelihood_part_e(\\\n",
    "            sensor_loc, obj_loc, distance, noise) \n",
    "        \n",
    "    return sensor_loc \n",
    " \n",
    " ########################################################################\n",
    "#########  Estimate distance given estimated sensor locations. ######### \n",
    "########################################################################\n",
    "\n",
    "def compute_distance_with_sensor_and_obj_loc(sensor_loc, obj_loc):\n",
    "    \"\"\"\n",
    "    stimate distance given estimated sensor locations.  \n",
    "    \n",
    "    Input:\n",
    "    sensor_loc: k * d numpy array. \n",
    "    Location of the sensors.\n",
    "    \n",
    "    obj_loc: n * d numpy array. Location of the n objects.\n",
    "    \n",
    "    Output:\n",
    "    distance: n * k dimensional numpy array. \n",
    "    \"\"\" \n",
    "    estimated_distance = scipy.spatial.distance.cdist(obj_loc, \n",
    "                                            sensor_loc, \n",
    "                                            metric='euclidean')\n",
    "    return estimated_distance \n",
    " \n",
    "########################################################################\n",
    "#########  Data Generating Functions ###################################\n",
    "########################################################################\n",
    "\n",
    "def generate_sensors(num_sensors = 7, spatial_dim = 2):\n",
    "    \"\"\"\n",
    "    Generate sensor locations. \n",
    "    Input:\n",
    "    num_sensors: The number of sensors.\n",
    "    spatial_dim: The spatial dimension.\n",
    "    Output:\n",
    "    sensor_loc: num_sensors * spatial_dim numpy array.\n",
    "    \"\"\"\n",
    "    sensor_loc = 100*np.random.randn(num_sensors,spatial_dim)\n",
    "    return sensor_loc\n",
    "\n",
    "def generate_dataset(sensor_loc, num_sensors = 7, spatial_dim = 2, \n",
    "                 num_data = 1, original_dist = True, noise = 1):\n",
    "    \"\"\"\n",
    "    Generate the locations of n points.  \n",
    "\n",
    "    Input:\n",
    "    sensor_loc: num_sensors * spatial_dim numpy array. Location of sensor. \n",
    "    num_sensors: The number of sensors.\n",
    "    spatial_dim: The spatial dimension.\n",
    "    num_data: The number of points.\n",
    "    original_dist: Whether the data are generated from the original \n",
    "    distribution. \n",
    "\n",
    "    Output:\n",
    "    obj_loc: num_data * spatial_dim numpy array. The location of the num_data objects. \n",
    "    distance: num_data * num_sensors numpy array. The distance between object and \n",
    "    the num_sensors sensors. \n",
    "    \"\"\"\n",
    "    assert num_sensors, spatial_dim == sensor_loc.shape\n",
    "\n",
    "    obj_loc = 100*np.random.randn(num_data, spatial_dim)\n",
    "    if not original_dist:\n",
    "       obj_loc += 1000\n",
    "\n",
    "    distance = scipy.spatial.distance.cdist(obj_loc, \n",
    "                                           sensor_loc, \n",
    "                                           metric='euclidean')\n",
    "    distance += np.random.randn(num_data, num_sensors) * noise\n",
    "    return distance, obj_loc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 117.77101717,   13.22460586,  390.17487407,  123.14341672,\n",
       "         237.46869444,  125.39343957,  135.79630445]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensorloc = generate_sensors()\n",
    "distance, obj_loc = generate_dataset(sensor_loc = sensorloc)\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
